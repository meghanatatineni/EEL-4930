---
title: "Predicting Diabetes in Pima Indian Population"
author: "Meghana Tatineni"
date: "10/01/2018"
output: 
  prettydoc::html_pretty:
    theme: tactile
---
##Statistical Modeling and Machine Learning in R 
Installing packages
library("tidyverse")
library("ggplot2")
library("caTools")
library("mice")
library("reshape2")
library("class")
library("randomForest")
library("caret")
```
The dataset is originally from National Institute of Diabetes and Digestive and Kidney Diseases. The purpose of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. 
This datset is restricted to females at least 21 years old of Pima Indian heritage. 
```{r}
#Always update your working directory 
library(RCurl)
x <- getURL("https://raw.githubusercontent.com/dsiufl/Workshop_development/master/diabetes.csv")
diab <- read.csv(text = x)
```

After loading our data, we are performing Exploratory Data Analysis to get a sense of what we are working with and how much data cleansing and wrangling we have to do. 
Usually 80% of the data science work is preparing the data for analysis and 20% is modeling our data.
Thankfully, this data set is already clean so we won't be doing much data prep. This will never be the case when working on a real data science project. 

```{r pressure, echo=FALSE}
head(diab)
str(diab)
summary(diab)
#number of NA values
sum(is.na.data.frame(diab))
```

There are no NA values in this dataset so we must have no missing data. Wrong! Looking at the first couple of rows, there are 0 values for Skin Thickness, Insulin, and Glucose which does not make sense.
Let's return the number of 0 values for each column to get an idea of how much of our data is missing. 
We are going to replace the 0 values with NA.
```{r}
colSums(diab == 0)
colSums(diab == 0)/768
#Replace 0 with NA 
diab<- diab  %>% select(-Insulin, -SkinThickness) %>% 
  mutate_each(funs(replace(.,.==0,NA)),-Outcome,-Pregnancies) 
```

To deal with the zero values, we will use Imputation. Imputation is just missing values with substituted values. We can replace missing values many ways such as using the median or mean. 
In general, there are two categories of missing data.
* MNAR:missing not at random
* MCAR:missing completely at random
For this data set, we are assuming the data is missing completely at random. 
We will be using the MICE package in R: Multivariate Imputation by Chained Equations using bayesian linear regression
Read more here:http://www.stefvanbuuren.nl/publications/mice%20in%20r%20-%20draft.pdf

```{r}
#pattern of missing values
library(mice)
md.pattern(diab)
#predictor matrix 
tempdata<-mice(diab,m=1,maxit =50, method = 'norm', seed='100')
```

Let's visualize our imputation result using density plots.
The density of the imputed data for each imputed dataset is showed in magenta while the density of the observed data is showed in blue. We expect the distributions to be similar.

```{r}
#return imputed values
tempdata$imp$Glucose
densityplot(tempdata)
#combine data sets 
diab_com<-mice::complete(tempdata,1)
#diab_com<- diab_com %>% mutate(Outcome = as.factor(Outcome))
```

Now that we fixed the missing values, lets continue our Exploratory Data Analysis and Visulization 
Let's get the summary statistics and distributions for each column. 
```{r}
summary(diab_com)
#distributions for each variable
library(reshape2)
library(ggplot2)
diab_melt<-melt(diab_com)
qplot(value, data=diab_melt) + facet_wrap(~variable, scales="free")
```

Let's look at the correlations between the features
```{r}
library("corrplot")
cor(diab_com)
corrplot(cor(diab_com), method="circle",type = "lower")
```

Distribution of Glucose for Diabetics and Non Diabetics
```{r}
library(ggplot2)
ggplot(diab_com,aes(x=Glucose,fill=factor(Outcome)))+geom_density(alpha=0.3)+scale_fill_manual(values=c("red", "blue"))+labs(title="Distribution of Glucose for Diabetics and Non Diabetics")
```


Before we start modeling, we need to split our data into a testing and training set. The purpose of this is to prevent overfitting.
Here we are randomly splitting our data into 75% traning set and 25% testing set. We use the training set to train our model and the testing set to determine the accuracy of the model.  
Read more here: 
```{r}
diab_com<- diab_com %>% mutate(Outcome = as.factor(Outcome))
set.seed(100)
library(caTools)
split<-sample.split(diab_com$Outcome, SplitRatio = .75)
train<-subset(diab_com,split==TRUE)
test<- subset(diab_com, split==FALSE)
```

###############################################################################################################
We can finally start the modeling our data using statistical methods and machine learning!

We will be comparing four different classification technquies by using supervised Machine Learning algorithms. Supervised learning means be already know the outcome of our data. For this data, we already know the person is either diabetic or not. 

We are using four Machine Learning models
1.Logistic Regression 
2.K Nearest Neighbors
3.Decision Trees
4.Random Forests 

1.Logistic Regression
Logistic regression is a simple statistical model which predicts a binary response (ex.YES/NO). For this data, we are predicting whether a woman is diabetic or not.
Read more here:https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc 

```{r}
fit_log<-glm(train$Outcome ~ ., data=train, family="binomial")
summary(fit_log)
predict_log<-predict(fit_log, test, type="response")
predict_log1<-ifelse(predict_log <.5, "0","1")
confusionMatrix(table(predict_log1,test$Outcome))
```

2.K Nearest Neighbors
non parametric - doesn't make any underlying assumptions about the data 
For each test data point, we would be looking at the K nearest training data points and take the most frequently occurring classes and assign that class to the test data.
Read more here:https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7 
```{r}
library(class)
model<-knn(train,test,k=10,cl=train$Outcome)
confusionMatrix(table(model,test$Outcome))
```

3.Decision Tree
tree that keeps splitting on features. splits the population based on most siginifcant factor 
Read more here:http://explained.ai/decision-tree-viz/index.html
```{r}
library(rpart.plot)
library(caret)
fit_tree<-rpart(Outcome~., data=train, method='class')
rpart.plot(fit_tree, extra = 106)
predict_tree<-predict(fit_tree,test,type='class')
confusionMatrix(predict_tree,test$Outcome)
```

4.Random Forest
A random forest is just a collection of decision trees. It prevents overfitting but it is hard to interpret and compuationally expensive
importance=importance of predictors 
Read more here:https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd
```{r}
library(randomForest)
fit_forest<-randomForest(Outcome~., data=train, importance=T, ntree=100)
predict_forest<-predict(fit_forest,test)
confusionMatrix(predict_forest, test$Outcome)
```

We can also use random forests to rank the importance of our features using the gini coefficient.
```{r}
importance(fit_forest)
varImpPlot(fit_forest, sort = T,main="Ranking Importance of Features",type=1)
```
Our best classfication rate is 81.5% using the logistic regression which is the simplest classification algorithm
To increase our classification rate, we can try tuning our parameters and try other classification algorithms such as support vector machines or neural networks.